<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[机器学习笔记之逻辑回归原理]]></title>
    <url>%2F2019%2F03%2F14%2Flogistic-fundamental%2F</url>
    <content type="text"><![CDATA[1.从线性回归到逻辑回归 线性回归的模型是求出输出特征向量Y和输入样本矩阵X之间的线性关系系数$θ$，满足$Y=Xθ$。此时我们的Y是连续的，所以是回归模型。 如果我们想要Y是离散的话, 一个可以想到的办法是，我们对于这个Y再做一次函数转换，变为$g(Y)$。如果我们令$g(Y)$的值在某个实数区间的时候是类别A，在另一个实数区间的时候是类别B，以此类推，就得到了一个分类模型。 2.二元逻辑回归的模型 逻辑回归的思想就是 在线性回归上再做一次函数转换，即上一节我们提到对线性回归的结果做一个在函数$g$上的转换，可以变化为逻辑回归。这个函数$g$在逻辑回归中我们一般取为sigmoid函数，形式如下： $$g(z)=\frac{1}{1+e^{-z}}$$ 图形如图： 它有一个非常好的性质，即当z趋于正无穷时，$g(z)$趋于1，而当$z$趋于负无穷时，$g(z)$趋于0，这非常适合于我们的分类概率模型。另外，它还有一个很好的导数性质： $$g^{`}(z)=g(z)(1-g(z))$$ 如果我们令$g(z)$中的$z$为：$z=xθ$，这样就得到了二元逻辑回归模型的一般形式：$$h_{\theta}(x)=\frac{1}{1+e^{\theta x}}$$ 其中$x$为样本输入，$hθ(x)$为模型输出，可以理解为某一分类的概率大小。而$θ$为分类模型的要求出的模型参数。 我们假设，如果$hθ(x)&gt;0.5$ ，即$xθ&gt;0$, 则$y$为1。如果$hθ(x)&lt;0.5$，即$xθ&lt;0$, 则$y$为0。 $y=0.5$是临界情况，此时$xθ=0$为， 从逻辑回归模型本身无法确定分类。 ——$hθ(x)$的值越小，而分类为0的的概率越高，反之，值越大的话分类为1的的概率越高。如果靠近临界点，则分类准确率会下降。 3.参数估计模型的数学形式确定后，剩下就是如何去求解模型中的参数。由于在线性回归模型中，输出$y$值是连续的，因此可以用差值的平方等表示损失函数。但是在回归中，输出$y$值是离散的，所以损失函数用极大似然法估计。 最大化似然函数 == 最小化损失函数 损失函数是似然函数求对数再取反 我们知道，按照二元逻辑回归的定义，假设我们的样本输出是0或1两类，那么我们有：$$P(y=1|x,\theta)=h_{\theta}(x)$$$$P(y=1|x,\theta)=1-h_{\theta}(x)$$把这两个式子写成一个式子，就是：$$P(y|x,\theta)=h_{\theta}(x)^y(1-h_{\theta}(x))^{1-y}$$其中y的取值只能是0或者1用矩阵法表示，即为：$$P(Y|X,\theta)=h_{\theta}(X)^Y(E-h_{\theta}(x))^{1-Y}$$其中E为单位矩阵。得到了y的概率分布函数表达式，我们就可以用似然函数最大化来求解我们需要的模型系数$\theta$.为了方便求解，这里我们用对数似然函数最大化，对数似然函数取反即为我们的损失函数$J(\theta)$.似然函数的代数表达式为：$$J(\theta)= -lnL(\theta) = -{\sum_{i=1}^m(y^{(i)}log(h_{\theta}(x^{(i)}))+(1-y^{(i)})log(1-h_{\theta}(x^{(i)})))}$$损失函数用矩阵法表达更加简洁：$$J(\theta)=- Y.logh_{\theta}(X)-(E-Y).log(E-h_{\theta}(X))$$这样，问题就转换成目标为最小化损失函数的优化问题，求解参数 4. 参数求解方法对于求解二元逻辑回归的损失函数极小化，有比较多的方法，最常见的有梯度下降法，坐标轴下降法，等牛顿法等。这里用梯度下降法中θ每次迭代的公式。（用矩阵的写法） 对于$J(\theta)=-Y.logh_{\theta}(X)-(1-Y).log(E-h_{\theta}(X))$,我们用$J(\theta)$对向量$\theta$向量求导得： $$\begin{align}\frac{\partial}{\partial \theta}J(\theta)=&amp;-YX^{T}\frac{1}{h_{\theta}(X)}h_{\theta}(X)(1-h_{\theta}(X))\\ &amp;+(E-Y)X^T\frac{1}{1-h_{\theta}(X)}h_{\theta}(X)(1-h_{\theta}(X))\end{align}$$这一步用到了矩阵求导的链式法则，和下面三个矩阵的求导公式： $\frac{\partial}{\partial X}logX=\frac{1}{X}$ $\frac{\partial}{\partial z}g(z)=g(z)(1-g(z))$ ($g(z)$为sigmoid函数) $\frac{\partial}{\partial \theta}X \theta= X^T(h_{\theta}(X)-Y)$ 从而在梯度下降法中每一步向量$ \theta $的迭代公式如下：$$\theta = \theta -\alpha X^T(h_{\theta}(X)-Y)$$其中，$ \alpha $为梯度下降法的步长 实践中，我们一般不用操心优化方法，大部分机器学习库都内置了各种逻辑回归的优化方法，不过了解至少一种优化方法还是有必要的。 5. 二元逻辑回归的正则化逻辑回归也会面临过拟合问题，所以我们也要考虑正则化。常见的有L1正则化和L2正则化 。二元逻辑回归的L1正则化损失函数表达式如下：$$J(\theta)=-Y.logh_{\theta}(X)-(E-Y).log(1-h_{\theta}(X))+\alpha||{\theta}||_1$$其中$\parallel{\theta}\parallel_1$ 为L1范数$$J(\theta)=-Y.logh_{\theta}(X)-(E-Y).log(1-h_{\theta}(X))+\alpha\parallel{\theta}\parallel_2^2$$其中$\parallel{\theta}\parallel_2^2$ 为L2范数两种正则化的特点L1正则化的模型建叫做Lasso回归，L1会趋向于产生少量的特征，而其他的特征都是0。Lasso在特征选择时候非常有用.L2正则化的模型叫做Ridge回归（岭回归）,L2会选择更多的特征，这些特征都会接近于0。这就只是一种规则化，防止过拟合。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[优雅地使用latex编辑数学公式]]></title>
    <url>%2F2019%2F03%2F13%2Flatex%2F</url>
    <content type="text"><![CDATA[在写博客的过程中发现写数学公式是一件非常不友好的事情，不仅繁琐而且常出现兼容问题。 这里介绍一个非常强大的工具Latex，通过简单的语法可以轻松写出美观优雅的公式。本博文总结了一些Latex的基础语法，让你5分钟即可轻松上手。 1. 排版方式行级元素(inline)，行级元素使用$...$，在正文行内使用，两个$表示公式的首尾。 块级元素(displayed)，块级元素使用$$...$$，单独成行、自动居中。 2. 常用西文符号小写字母\alpha, \beta, …, \omega代表α,β,…ω. 大写字母,使用\Gamma, \Delta, …, \Omega代表Γ,Δ,…,Ω. 3. 上标与下标使用 ^ 和 _ 表示上标和下标. 例如,x_i^2:$x_i^2$ ，\log_2x:$\log_2x$ 使用{ }来保证优先级问题。例如要显示$10^{10}$,正确的语法应该是10^{10},若写成10^10则会显示成$10^10$。 4. 括号小括号和中括号直接使用，大括号由于因为用来表示优先级，所以需要转义。\{1+2\}:${1+2}$ 5. 运算 分数：\frac{}{},例如，\frac{1+1}{2}+1: $\frac{1+1}{2}+1$ 求和：\sum_1^n: $\sum_1^n$ 连乘：\prod_1^n: $\prod_1^n$ 积分：\int_1^n: $\int_1^n$ 极限：\lim_{x \to \infty}: $\lim_{x \to \infty}$ 矩阵：$$\begin{matrix}…\end{matrix}$$，使用&amp;分隔同行元素，\换行。例如：\begin{matrix}1 &amp; x &amp; x^2 \\1 &amp; y &amp; y^2 \\1 &amp; z &amp; z^2 \\\end{matrix},则显示为：$$\begin{matrix}1 &amp; x &amp; x^2 \\1 &amp; y &amp; y^2 \\1 &amp; z &amp; z^2 \\\end{matrix}$$ 6. 顶标与底标 角号：\hat{a}: $\hat{a}$ 横线：\overline{a}: $\overline{a}$ 箭头：\stackrel{\rightarrow}{a}: $\stackrel{\rightarrow}{a}$ 7. 集合关系 属于：\in: $\in$ 不属于：\not\in: $\not\in$ 包含于：A\subset B: $A\subset B$ 不包含于：A\not\subset B: $A\not\subset B$ 交：A\cap B: $A\cap B$ 并：A\cup B: $A\cup B$ 空集：\emptyset: $\emptyset$ 8. 例子h(\theta)=\sum_{j=0}^n \theta_jx_j$$h(\theta)=\sum_{j=0}^n \theta_jx_j$$ J(\theta)=\frac1{2m}\sum_{i=0}(y^i-h_\theta(x^i))^2$$J(\theta)=\frac1{2m}\sum_{i=0}(y^i-h_\theta(x^i))^2$$ 12345f(n) = \begin&#123;cases&#125; n/2, &amp; \text&#123;if $n$ is even&#125; \\\\ 3n+1, &amp; \text&#123;if $n$ is odd&#125; \end&#123;cases&#125; $$f(n) = \begin{cases} n/2, &amp; \text{if $n$ is even} \\ 3n+1, &amp; \text{if $n$ is odd} \end{cases}$$ 123456\begin&#123;align&#125;\frac&#123;\partial J(\theta)&#125;&#123;\partial\theta_j&#125;&amp; = -\frac1m\sum_&#123;i=0&#125;^m(y^i-h_\theta(x^i)) \frac&#123;\partial&#125;&#123;\partial\theta_j&#125;(y^i-h_\theta(x^i))\\\\&amp; = -\frac1m\sum_&#123;i=0&#125;^m(y^i-h_\theta(x^i)) \frac&#123;\partial&#125;&#123;\partial\theta_j&#125;(\sum_&#123;j=0&#125;^n\theta_jx_j^i-y^i)\\\\&amp; = -\frac1m\sum_&#123;i=0&#125;^m(y^i-h_\theta(x^i))x^i_j\end&#123;align&#125; $$\begin{align}\frac{\partial J(\theta)}{\partial\theta_j}&amp; = -\frac1m\sum_{i=0}^m(y^i-h_\theta(x^i)) \frac{\partial}{\partial\theta_j}(y^i-h_\theta(x^i))\\&amp; = -\frac1m\sum_{i=0}^m(y^i-h_\theta(x^i)) \frac{\partial}{\partial\theta_j}(\sum_{j=0}^n\theta_jx_j^i-y^i)\\&amp; = -\frac1m\sum_{i=0}^m(y^i-h_\theta(x^i))x^i_j\end{align}$$ 附录latex常用符号大全]]></content>
      <categories>
        <category>善其事利其器</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据可视化之Matplotlib:Pandas中的绘图函数]]></title>
    <url>%2F2019%2F03%2F13%2F20190313-python-pandas-matplotlib%2F</url>
    <content type="text"><![CDATA[Pandas有许多能够利用DataFrame对象数据组织特点来创建标准图表的高级绘图方法，本文主要介绍的是pandas中的绘图函数。 12345#coding:utf-8import matplotlib.pyplot as pltimport pandas as pdimport numpy as npfrom pandas import DataFrame,Series 1. 线形图&emsp;df.plot( kind=&#39;line&#39;) Series 的plot 方法会以index作为X轴,画一条线 DataFrame 的plot 方法会以index作为X轴，给每一列绘制一条线,columns作为图例。 12345678910111213141516#第一种创建画布和画布分区方法，分开创建figure和subplot对象fig=plt.figure()#Series 的线形图ax1=fig.add_subplot(2,1,1)s=Series(np.random.rand(10).cumsum(),index=np.arange(0,100,10)) s.plot( kind='line')plt.xlabel(u"index") plt.title(u"Serise的线形图")plt.show()#DataFrame的线形图ax2=fig.add_subplot(2,1,2)df=DataFrame(np.random.rand(10,4).cumsum(0),index=np.arange(0,100,10),columns=pd.Index(['A','B','C','D'],name='Genus'))df.plot( kind='line')plt.xlabel(u"index") plt.title(u"DataFrame的线形图")plt.show() 其中，Series.plot方法的参数 kind ：图的类型，‘line’,’bar’,’barh’,’kde’ label ：图例标签 ax ：需要绘制的对象 rot ：旋转角度 xticks ：X轴刻度值 xlim ：X轴刻度范围 grid ：显示网格 2. 柱状图&emsp;df.plot( kind=&#39;bar&#39;) : 垂直柱状图 &emsp;df.plot( kind=&#39;barh&#39;) : 水平柱状图 &emsp;df.plot( kind=&#39;bar&#39;,stacked=True) : stacked属性为True可以设置为堆积柱状图 Series的柱状图123456#第二种创建画布和画布分区方法，创建figure，返回一个subplot对象fig,axes =plt.subplots(2,1) s=Series(np.random.rand(16),index=list('abcdefghijklmnop')) s.plot( kind= 'bar' ,ax=axes[0]) #返回的axes的数组可指定在哪个subplot对象上画图s.plot( kind= 'barh' ,ax=axes[1]) plt.show() DataFrame的柱状图 每一行的值为一组，每一列的columns为图例 12345fig,axes =plt.subplots(2,1) df=DataFrame(np.random.rand(4,4),index=['one','two','three','four'],columns=pd.Index(['A','B','C','D'],name='Genus')) df.plot( kind= 'bar',ax=axes[0] ) df.plot( kind= 'bar',ax=axes[1],stacked=True ) #stacked=True 可以生成堆积柱状图plt.show() 3.密度图( KDE, Kernel Density Estimate ,核密度估计 ) 密度图即为连续概率分布图，将分布近似为标准混合正态分布。 &emsp;df.plot( kind=&#39;kde&#39;) 1234567891011121314fig=plt.figure()#Series 的密度图fig.add_subplot(2,1,1)s=Series(np.random.rand(50).cumsum(),index=np.arange(0,100,2)) s.plot(kind='kde')plt.title(u"Series的密度图")plt.show()#DataFrame 的密度图，会给每一列都画一条密度估计线，并将columns自动生成图例fig.add_subplot(2,1,2)df=DataFrame(np.random.rand(10,4).cumsum(0),index=np.arange(0,100,10),columns=pd.Index(['A','B','C','D'],name='Genus'))df.plot(kind='kde')plt.title(u"DataFrame的密度图")plt.show() 4.直方图 直方图是对值频率进行离散化显示的柱状图，数据点呗拆分到离散的、间隔均匀的面元中。 &emsp;df.hist( bins=10) :bins属性可设置柱子数量 12345678910111213fig=plt.figure()#Series 的密度图fig.add_subplot(2,1,1)s=Series(np.random.rand(20).cumsum(),index=np.arange(0,100,5)) s.hist( bins=10)plt.title(u"Series的直方图")plt.show()#DataFrame 的密度图，会给每一列都画一张直方图，并将列名作为对应标题fig.add_subplot(2,1,2)df=DataFrame(np.random.rand(10,4).cumsum(0),index=np.arange(0,100,10),columns=pd.Index(['A','B','C','D'],name='Genus'))df.hist( bins=10)plt.show() 5.散点图 散点图是观察两个一维数据间关系的有效方式 &emsp;plt.scatter(X,Y) 1234567macro = pd.read_csv('macrodata.csv')data = macro[['cpi','m1','tbilrate','unemp']]trans_data = np.log(data).diff().dropna()# 画出两个Series之间的散点图plt.scatter(trans_data['m1'],trans_data['unemp'])plt.title('Change in log %s vs.log %s' % ('m1','unemp'))plt.show() DataFrame的散点图矩阵 创建散布图矩阵，会把DataFrame中任意两列画散点图，观察其之间的关系。 支持在对角线上放置各变量的直方图或者密度图 &emsp;pd.scatter_matrix( trans_data ,diagonal = &#39;kde&#39; ,color =&#39;k&#39; ,alpha=0.3) 123# 画出散布图矩阵pd.plotting.scatter_matrix( trans_data ,diagonal = 'kde' ,color ='k' ,alpha=0.3)plt.show()]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F03%2F06%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
